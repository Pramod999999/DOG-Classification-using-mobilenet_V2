# -*- coding: utf-8 -*-
"""DeepLearning_classification_dogbreed.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q8BBgMD2jln2Fm6H-jL2FVh5puFVknXp
"""

import tensorflow as tf

import tensorflow_hub as hub

import pandas as pd
data= pd.read_csv("/content/drive/MyDrive/Dog_breed/dog-breed-identification/labels.csv")
data

data.info()

#Checking how many dogs are the in a particular breed for all the  breeds
data["breed"].value_counts()

#checking the effeciency of data i.e min every
print(data["breed"].value_counts().median())
(data["breed"].value_counts().mean())

#Importing images of dogs from training data and accessing them to check whther they are proper, training data have labels
from IPython.display import Image
Image ( "/content/drive/MyDrive/Dog_breed/dog-breed-identification/train/000bec180eb18c7604dcecc8fe0dba07.jpg")
#/content/drive/MyDrive/Dog_breed/dog-breed-identification/labels.csv

"""Creating Array of Image id's address so that it will be useful for handelling

"""

filenames = ["/content/drive/MyDrive/Dog_breed/dog-breed-identification/train/"+ a + ".jpg" for a in data["id"]]
filenames

#Checking whether the length of array and length of original image set is same or not
len(filenames)

type(filenames)

import os
len(os.listdir("/content/drive/MyDrive/Dog_breed/dog-breed-identification/train/"))

from IPython.display import Image
Image(filenames[4])
#Image(filenames[30])
for i in range(5) :
  print(Image(filenames[i]))

filenames[30]

data["breed"][3500]

import numpy as np
breed = np.array(data["breed"])
breed

len(breed) #Crosschecking whether length of breed is equal to length of filenames(which we have created )

#Checking number of different type of lables to predict
unique_breeds= np.unique(breed)

unique_breeds

len (np.unique(breed))

len(unique_breeds)

"""The length of unique_breeds should be 120, meaning we're working with images of 120 different breeds of dogs and our model has to identify between them and we have to train it accordingly"""

#Turning a breed into array of boolean
print(breed[0])
breed[0] == unique_breeds

"""By using this we converted a particular dog breed(predicator variable) into array of boolean :"""

from tensorflow.python.ops.gen_array_ops import unique
# Turnning every label(breed) into a boolean array
boolean_labels = [i == unique_breeds for i in breed]

boolean_labels[:3]

len(boolean_labels)

# @title Default title text
print(breed[0]) # original label
#len(unique_breeds)
#for j in unique_breeds :
#  for i in range(0,120) :
#   if  j[i] == breed[0]:
#     print (i)
print(np.where(unique_breeds == breed[0])[0][0]) # index where label occurs
print(boolean_labels[0].argmax()) # index where label occurs in boolean array
print(boolean_labels[0].astype(int)) # there will be a 1 where the sample label occurs

"""Creating our own valiadiation set since in kaggle they did'nt give any valiadiation set"""

#Since we have 10,000 images to train , it will take lot of time to train it , so starting it with less no of image items , say 1000
x= filenames
y= boolean_labels

len(x)

len(boolean_labels)

NUM_IMAGES = 1000 #@param{type:"slider",min:1000,max:10000,step:1000}
NUM_IMAGES

#Now Splitting data into testing and valiadation set
from sklearn.model_selection import train_test_split
x_train,x_val,y_train,y_val = train_test_split(x[:NUM_IMAGES],y[:NUM_IMAGES],test_size = 0.2 , random_state =87 )

x_train

y_train

x_val

y_val

"""Converting images into Numpy arrays and then into tensors"""

from matplotlib.pyplot import imread
Image = imread(filenames[895])
Image.shape
Image

Image

tf.constant(Image ) #converting image into tensors

tf.constant(filenames)
data = tf.data.Dataset.from_tensor_slices((tf.constant(filenames)))
data

"""Creating a function which takes Image path as input and returns modified image after Pre Processing. To preprocess images into Tensors, creating a function which does

1.Takes an image filename as input.

2.Uses TensorFlow to read the file and save it to a variable, image.

3.Turn our image (a jpeg file) into Tensors.

4.Resize the image to be of shape (224, 224).

5.Return the modified image.
"""

# Define image size
IMG_SIZE = 224

def process_image(image_path):
  """
  Takes an image file path and turns it into a Tensor.
  """
  # Read in image file
  image = tf.io.read_file(image_path)
  # Turn the jpeg image into numerical Tensor with 3 colour channels (Red, Green, Blue)
  image = tf.image.decode_jpeg(image, channels=3)
  # Convert the colour channel values from 0-225 values to 0-1 values
  image = tf.image.convert_image_dtype(image, tf.float32)
  # Resize the image to our desired size (224, 244)
  image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])
  return image

"""Making data into batches , Since using all the data to train at a time is not efficient and might not fit into memory while training . To make into batches we need data in a form of Tensor tuple

"""

#Creating a function which returns a Tensortuple
def get_image_label(image_path,breed):
  image= process_image(image_path)
  return image,breed

#Testing the above functions
get_image_label(filenames[367],y[367])

BATCH_SIZE=32
#Creating a function which convert all our data in batches
def create_data_batches(x, y=None, batch_size=32, valid_data=False, test_data=False):
  """
  Creates batches of data out of image (x) and label (y) pairs.
  Shuffles the data if it's training data but doesn't shuffle it if it's validation data.
  Also accepts test data as input (no labels).
  """
  # If the data is a test dataset, it  don't have labels
  if test_data:
    print("Creating test data batches...")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(x)))
    data_batch = data.map(process_image).batch(32)
    return data_batch

  # If the data if a valid dataset, we don't need to shuffle it
  elif valid_data:
    print("Creating validation data batches...")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(x),
                                               tf.constant(y)))
    data_batch = data.map(get_image_label).batch(32)
    return data_batch

  else:
    # If the data is a training dataset, we shuffle it
    print("Creating training data batches...")
    # Turn filepaths and labels into Tensors
    data = tf.data.Dataset.from_tensor_slices((tf.constant(x),
                                              tf.constant(y)))

    # Shuffling pathnames and labels before mapping image processor function is faster than shuffling images
    data = data.shuffle(buffer_size=len(x))

    # Create (image, label) tuples (this also turns the image path into a preprocessed image)
    data = data.map(get_image_label)

    # Turn the data into batches
    data_batch = data.batch(BATCH_SIZE)
  return data_batch

# Create training and validation data batches
train_data = create_data_batches(x_train, y_train)
val_data = create_data_batches(x_val, y_val, valid_data=True)

#Visulizating the data
import matplotlib.pyplot as plt

# Create a function for viewing images in a data batch
def show_25_images(images, labels):
  """
  Displays 25 images from a data batch.
  """
  # Setup the figure
  plt.figure(figsize=(20, 20))
  # Loop through 25 (for displaying 25 images)
  for i in range(25):
    # Create subplots (5 rows, 5 columns)
    ax = plt.subplot(5, 5, i+1)
    # Display an image
    plt.imshow(images[i])
    # Add the image label as the title
    plt.title(unique_breeds[labels[i].argmax()])
    # Turn gird lines off
    plt.axis("off")

# Visualizing training images from the training data batch
train_images, train_labels = next(train_data.as_numpy_iterator()) #To unbatch and iterate the data
show_25_images(train_images, train_labels)

#We are going to use transfer learning to do this because bulding a model from scratch needs lot of data(might be in millions) to develop and to train with millions of data takes lot of lot of time and cost .

# Setup input shape to the model
INPUT_SHAPE = [None, IMG_SIZE, IMG_SIZE, 3] # batch, height, width, colour channels

# Setup output shape of the model
OUTPUT_SHAPE = len(unique_breeds) # number of unique labels

# Setup model URL from TensorFlow Hub
MODEL_URL = "https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4"

# Creating a function which builds a Keras model
def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):
  print("Building model with:", MODEL_URL)

  # Setup the model layers
  model = tf.keras.Sequential([
    hub.KerasLayer(MODEL_URL), # Layer 1 (input layer)
    tf.keras.layers.Dense(units=OUTPUT_SHAPE,
                          activation="softmax") # Layer 2 (output layer)
  ])

  # Compiling the model
  model.compile(
      loss=tf.keras.losses.CategoricalCrossentropy(),
      optimizer=tf.keras.optimizers.Adam(),
      metrics=["accuracy"] # We'd like this to go up
  )

  # Building the model by giving an input shape
  model.build(INPUT_SHAPE)

  return model

model=create_model()
model.summary()

"""we are going to use 2 call backs
1. tensorBoard call back
2.EarlyStoppage call back
"""

# Commented out IPython magic to ensure Python compatibility.
#To load Tensorboard extension using magic function

# %load_ext tensorboard

import datetime

# Creating  function to build a TensorBoard callback
def create_tensorboard_callback():
  #Creating a log directory for storing TensorBoard logs which is stored with thee info of date and time
  logdir = os.path.join("drive/My Drive/Data/logs", datetime.datetime.now().strftime("%d%m%Y-%H%M%S"))
  return tf.keras.callbacks.TensorBoard(logdir)

  TensorBoard_callback = tf.keras.callbacks.TensorBoard(logdir = os.path.join("/content/drive/MyDrive/Dog_breed/dog-breed-identification/logs", datetime.datetime.now().strftime("%d%m%Y-%H%M%S")))

#Creating early stopping callback

early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_accuracy",patience=3) # stops after 3 rounds if there is no improvement in model

#Traning our model
NUM_EPOCHS = 100 #@param {type:"slider", min:10, max:100, step:10}

#Creating a function to train and return trained model by testing on validation data
def train_model():


  model = create_model()

  # Creating new TensorBoard session everytime we train a model
  #tensorboard = create_tensorboard_callback()

  # Fitting the model
  model.fit(x=train_data,
            epochs=NUM_EPOCHS,
            validation_data=val_data,
            validation_freq=1, # check validation metrics every epoch
            callbacks=[create_tensorboard_callback(), early_stopping])
  return model

model = train_model()

# Commented out IPython magic to ensure Python compatibility.
#%tensorboard --logdir drive/My\ Drive/Data/logs
# %tensorboard --logdir /content/drive/MyDrive/Data/logs

predictions = model.predict(val_data)

predictions

np.argmax(predictions[4])

unique_breeds[53]

unique_breeds[np.argmax(predictions[4])]

#Turning prediction probabilities into their respective label (easier to understand)
def get_pred_label(prediction_probabilities):
  """
  Turns an array of prediction probabilities into a label.
  """
  return unique_breeds[np.argmax(prediction_probabilities)]

pred_label = get_pred_label(predictions[46])
pred_label

val_data

"""Now since our val_data is in the form of Batches we have to unbatchify to predict by itreating over it.

"""

# Createing a function to unbatch a batched dataset
def unbatchify(data):
  """
  Takes a batched dataset of (image, label) Tensors and reutrns separate arrays
  of images and labels.
  """
  images = []
  labels = []
  # Looping through unbatched data
  for image, label in data.unbatch().as_numpy_iterator():
    images.append(image)
    labels.append(unique_breeds[np.argmax(label)])
  return images, labels

# Unbatchify the validation data
val_images, val_labels = unbatchify(val_data)
val_images[0], val_labels[0]

#Plotting Predications
def plot_pred(prediction_probabilities, labels, images, n=1):
  """
  View the prediction, ground truth and image for sample n
  """
  pred_prob, true_label, image = prediction_probabilities[n], labels[n], images[n]

  # Get the pred label
  pred_label = get_pred_label(pred_prob)

  # Plot image & remove ticks
  plt.imshow(image)
  plt.xticks([])
  plt.yticks([])

  # Change the colour of the title depending on if the prediction is right or wrong
  if pred_label == true_label:
    color = "green"
  else:
    color = "red"

  # Change plot title to be predicted, probability of prediction and truth label
  plt.title("{} {:2.0f}% {}".format(pred_label,
                                    np.max(pred_prob)*100,
                                    true_label),
                                    color=color)

plot_pred(prediction_probabilities=predictions,
          labels=val_labels,
          images=val_images,
          n=82)

def plot_pred_conf(prediction_probabilities, labels, n=1):
  """
  Plus the top 10 highest prediction confidences along with the truth label for sample n.
  """
  pred_prob, true_label = prediction_probabilities[n], labels[n]

  # Get the predicted label
  pred_label = get_pred_label(pred_prob)

  # Find the top 10 prediction confidence indexes
  top_10_pred_indexes = pred_prob.argsort()[-10:][::-1]
  # Find the top 10 prediction confidence values
  top_10_pred_values = pred_prob[top_10_pred_indexes]
  # Find the top 10 prediction labels
  top_10_pred_labels = unique_breeds[top_10_pred_indexes]

  # Setup plot
  top_plot = plt.bar(np.arange(len(top_10_pred_labels)),
                     top_10_pred_values,
                     color="grey")
  plt.xticks(np.arange(len(top_10_pred_labels)),
             labels=top_10_pred_labels,
             rotation="vertical")

  # Change color of true label
  if np.isin(true_label, top_10_pred_labels):
    top_plot[np.argmax(top_10_pred_labels == true_label)].set_color("green")
  else:
    pass

plot_pred_conf(prediction_probabilities=predictions,
               labels=val_labels,
               n=78)

# Creating a function to save a model
def save_model(model, suffix=None):
  """
  Saves a given model in a models directory and appends a suffix (string).
  """
  # Create a model directory pathname with current time
  modeldir = os.path.join("/content/drive/MyDrive/Data/models",
                          datetime.datetime.now().strftime("%d%m%Y-%H%M%s"))
  model_path = modeldir + "-" + suffix + ".h5" # save format of model
  print(f"Saving model to: {model_path}...")
  model.save(model_path)
  return model_path

# Creating a function to load saved model
def load_model(model_path):
  """
  Loads a saved model from a specified path.
  """
  print(f"Loading saved model from: {model_path}")
  model = tf.keras.models.load_model(model_path,
                                     custom_objects={"KerasLayer":hub.KerasLayer})
  return model

save_model(model, suffix="1000-images-mobilenetv2-new")

#Loading the saved model
loaded_model= load_model("/content/drive/MyDrive/Data/models/06012024-21181704575891-1000-images-mobilenetv2-new.h5")

#Using model for testing data, our testing data only has images

len(x)

full_data =  create_data_batches(x,y)

full_model = create_model()

# Creating full model callbacks
full_model_tensorboard = create_tensorboard_callback()
# No validation set when training on all the data, so we can't monitor validation accuracy
full_model_early_stopping = tf.keras.callbacks.EarlyStopping(monitor="accuracy",
                                                             patience=3)

# Fit the full model to the full data
full_model.fit(x=full_data,
               epochs=NUM_EPOCHS,
               callbacks=[full_model_tensorboard, full_model_early_stopping])

save_model(full_model, suffix="full-image-set-mobilenetv2-new")

#Making predictions on the test dataset
#To make predications on test data we have change test data into same form as how we trained the model

# Load test image filenames
test_path = "/content/drive/MyDrive/Dog_breed/dog-breed-identification/test/"
test_filenames = [test_path + i for i in os.listdir(test_path)]

test_filenames[:10]

test_data = create_data_batches(test_filenames, test_data=True)

test_data

test_predictions = full_model.predict(test_data,verbose=1)

#Saving predictions(NumPy array) to csv file (for access later)

np.savetxt("/content/drive/MyDrive/Dog_breed/dog-breed-identification/Predications.csv", test_predictions, delimiter=",")

# Load predictions (NumPy array) from csv file
test_predictions = np.loadtxt("/content/drive/MyDrive/Dog_breed/dog-breed-identification/Predications.csv", delimiter=",")

test_predictions[:10]

#Storing predications in a from of dataset
# Creating a pandas DataFrame with empty columns
preds_df = pd.DataFrame(columns=["id"] + list(unique_breeds))
preds_df.head()

# Append test image ID's to predictions DataFrame
test_ids = [os.path.splitext(path)[0] for path in os.listdir(test_path)]
preds_df["id"] = test_ids

preds_df.head()

# Add the prediction probabilities to each dog breed column
preds_df[list(unique_breeds)] = test_predictions
preds_df.head()

